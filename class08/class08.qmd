---
title: "Class 08: Mini Project Breast Cancer"
author: "Nathan Joseph (PID: A17668656)"
format: pdf
toc: TRUE
---

### Background
In today's class we will be employing all R techniques for data analysis that we have learned thus far - including the machine learning methods of clustering and PCA - to analyze real breast cancer biopsy data.

### Data Import 

Data from the Wisconsin Cancer CSV file:
```{r}
fna.data <- "WisconsinCancer.csv"

wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df, 3)
```

Removing the first column from the dataframe as this is the pathologist provided diagnosis which is the answer to if our cell samples are malignant or benign:

```{r}
wisc.data <- wisc.df[,-1]
```

Creating diagnosis vector from the original dataset:
```{r}
diagnosis <- wisc.df$diagnosis
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```
There are 569 observations in this dataset.

> Q2. How many of the observations have a malignant diagnosis?

```{r}
count = 0
for (i in 1:nrow(wisc.data)){
  if (diagnosis[i]=="M"){
    count = count + 1
  }
}
count
```
There are 212 observations with a malignant diagnosis in this dataset.

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
sum(endsWith(colnames(wisc.data), "_mean"))
```
There are 10 features in the data that are suffixed with "_mean".

### Principal Component Analysis (PCA)
The main function in base R called `prcomp()` we will use the optional argument `scale=TRUE` here as the data columns/features/dimensions are on very different scales in the original dataset.

Perform PCA on wisc.data by completing the following code:

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
attributes(wisc.pr)
```

```{r}
library(ggplot2)

ggplot(wisc.pr$x) + aes(PC1, PC2, col = diagnosis) + geom_point()
```


Let's check the mean and standard deviation to see if scaling is necessary:
```{r}
colMeans(wisc.data)
```

```{r}
apply(wisc.data,2,sd)
```


View the summary:
```{r}
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

```{r}
var_explained <- wisc.pr$sdev^2/sum(wisc.pr$sdev^2)
var_explained[1]
```

The first principal component captures 44.27% of the original variance.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
var_explained <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2) 
cumulative <- cumsum(var_explained) 
which(cumulative >= 0.70)[1]
```

Three principal components are required to describe at least 70% of the original variance in the data.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
var_explained <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2) 
cumulative <- cumsum(var_explained) 
which(cumulative >= 0.90)[1]
```

Seven principal components are required to describe at least 90% of the original variance in the data.

>Q7. What stands out to you about this plot? Is it easy or di￿cult to understand? Why?

A quick bioplot of the principal components:
```{r}
biplot(wisc.pr)
```
This plot shows a lot of features, dimensions, and a lot of numbers from the 2 principal components.
Plotting PC1 vs PC2:
```{r}
library(ggplot2)

ggplot(as.data.frame(wisc.pr$x)) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(as.data.frame(wisc.pr$x)) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```
In these two plots, the principal component analysis shows that the principal components are identifying divisions in the data. In the above plot, the points are lower and more clustered towards the bottom.

```{r}
pr.var <- wisc.pr$sdev^2

head(pr.var)
```
Plotting the variance explained by each principal component:
```{r}
pve <- pr.var / sum(pr.var)
plot(c(1,pve), xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation[, 1]
```
The concave points means, which is acomponent of the loading vector, contributes the most to the `wisc.pr` data set in the first column. There are not any functions that are higher than the concave.points_mean.

## 4. Hierarchial Clustering

The goal of this section is to do hierarchical clustering of the original data to see if there is any obvious grouping into malignant and benign clusters.

```{r}
data.scaled <- scale(wisc.data) 
data.dist <- dist(data.scaled, method = "euclidian") 
wisc.hclust <- hclust(data.dist, method = "complete")
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 2) 
table(wisc.hclust.clusters)
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="red", lty=2)
```
The clustering model has 4 clusters at the height of 20.

> Q12. Which method gives your favorite results for the same data.dist dataset?
Explain your reasoning.

I prefer the cluster dendrogram as is more detailed towards the original nodes of the dataset and also it best visualizes accuracy and variance in the dataset.

## Combining Methods

The idea here is that I can take my new variables (the PCs) that are better descriptors of the dataset than the original features (i.e. the 30 columns in wisc.data) and use these as a basis for clustering


```{r}
pc.dist <- dist(wisc.pr$x[ ,1:3])
wisc.pr.hclust <- hclust (pc.dist, method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k = 2) 
table(grps)
```

I can now run `table()` with both my clustering `grps` and the expert `diagnosis`

```{r}
table(grps, diagnosis)
```

> Q13. How well does the newly created hclust model with two clusters separate out
the two “M” and “B” diagnoses?

The newly created hclust model with two cluster separates the two "M" and "B" diagnoses pretty well as demonstrated in more closed form dendrogram. The dendrogram is also much simpler.

Our cluster “1” has 179 “M” diagnosis 
Our cluster “2” has 333 “B” diagnosis 
179 TP 24 FP 333 TN 24 FN 
Sensitivity: TP/(TP + FN) 

```{r}
179/(179+33)
```

Specificity: TN/(TN+FP)

```{r}
333/(333+24)
```

## Prediction

We can use our PCA model for prediction of new un-seen cases.

```{r}
url <- "https://tinyurl.com/new-samples-CSV" 
new <- read.csv(url) 
npc <- predict(wisc.pr, newdata=new) 
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps) 
points(npc[,1], npc[,2], col="blue", pch=16, cex=3) 
text(npc[,1], npc[,2], c(1,2), col="white")
```
> Q16. Which of these new patients should we prioritize for follow up based on your results?

Since there is a tighter cluster around patient 1, this is the patient we should prioritize for a follow up based on the results as there is a large variation or spread around patient 2.
